{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":1104205,"sourceType":"datasetVersion","datasetId":618181}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)\n    for filename in filenames:\n        if filename != '*.jpg':\n            print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:58:43.539424Z","iopub.execute_input":"2025-03-13T10:58:43.539811Z","iopub.status.idle":"2025-03-13T10:59:21.729810Z","shell.execute_reply.started":"2025-03-13T10:58:43.539780Z","shell.execute_reply":"2025-03-13T10:59:21.728649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.autograd.set_detect_anomaly(True)\n\nprint('pytorch version:', torch.__version__)\nprint(\"GPU available:\", torch.cuda.device_count())\n#print('GPU name:',torch.cuda.get_device_name(0))\ndevice_name = (torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\")\nprint(\"device name:\", device_name)\n\n# Set the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:59:21.731268Z","iopub.execute_input":"2025-03-13T10:59:21.731604Z","iopub.status.idle":"2025-03-13T10:59:26.524824Z","shell.execute_reply.started":"2025-03-13T10:59:21.731572Z","shell.execute_reply":"2025-03-13T10:59:26.523366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For data augmentation\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import v2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:02:08.673900Z","iopub.execute_input":"2025-03-13T11:02:08.675900Z","iopub.status.idle":"2025-03-13T11:02:12.558557Z","shell.execute_reply.started":"2025-03-13T11:02:08.675844Z","shell.execute_reply":"2025-03-13T11:02:12.557383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\n\n# For Tokenizers\nfrom transformers import ViTImageProcessor, ViTConfig\n\n# For Model\nfrom transformers import ViTModel, ViTForImageClassification\n\n# For GPU\nfrom transformers import set_seed\nfrom torch.optim import AdamW\nfrom accelerate import Accelerator, notebook_launcher\n\n# For Dataset\nfrom torch.utils.data import Dataset, DataLoader\n\n# For Loss calculation\nimport torch.nn.functional as F\nfrom torch.nn import CosineEmbeddingLoss, TripletMarginLoss, MSELoss\n\n# For Display\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:16:05.689027Z","iopub.execute_input":"2025-03-13T12:16:05.689481Z","iopub.status.idle":"2025-03-13T12:16:05.695347Z","shell.execute_reply.started":"2025-03-13T12:16:05.689444Z","shell.execute_reply":"2025-03-13T12:16:05.694177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom itertools import combinations, product\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [8,5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:22:37.613834Z","iopub.execute_input":"2025-03-13T11:22:37.614216Z","iopub.status.idle":"2025-03-13T11:22:37.619592Z","shell.execute_reply.started":"2025-03-13T11:22:37.614187Z","shell.execute_reply":"2025-03-13T11:22:37.618285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:02:19.968961Z","iopub.execute_input":"2025-03-13T11:02:19.969325Z","iopub.status.idle":"2025-03-13T11:02:19.990228Z","shell.execute_reply.started":"2025-03-13T11:02:19.969295Z","shell.execute_reply":"2025-03-13T11:02:19.988646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Declare Global Constants","metadata":{}},{"cell_type":"code","source":"output_dir = '/kaggle/working'\ndata_dir = '/kaggle/input/celeba-dataset'\nimage_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:40:59.044128Z","iopub.execute_input":"2025-03-13T11:40:59.044556Z","iopub.status.idle":"2025-03-13T11:40:59.049538Z","shell.execute_reply.started":"2025-03-13T11:40:59.044525Z","shell.execute_reply":"2025-03-13T11:40:59.048002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 8\nprint('BATCH_SIZE =',BATCH_SIZE)\n\nMODEL_TRANSFORMER = 'google/vit-base-patch16-224'\n\nCLIP_SIZE = 224\nprint('Image Dimension =', CLIP_SIZE,'X', CLIP_SIZE)\n\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:03:29.721427Z","iopub.execute_input":"2025-03-13T11:03:29.721813Z","iopub.status.idle":"2025-03-13T11:03:29.728726Z","shell.execute_reply.started":"2025-03-13T11:03:29.721782Z","shell.execute_reply":"2025-03-13T11:03:29.727379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:03:29.909284Z","iopub.execute_input":"2025-03-13T11:03:29.909802Z","iopub.status.idle":"2025-03-13T11:03:29.915274Z","shell.execute_reply.started":"2025-03-13T11:03:29.909765Z","shell.execute_reply":"2025-03-13T11:03:29.914007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_everything(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:03:30.108121Z","iopub.execute_input":"2025-03-13T11:03:30.108535Z","iopub.status.idle":"2025-03-13T11:03:30.115315Z","shell.execute_reply.started":"2025-03-13T11:03:30.108499Z","shell.execute_reply":"2025-03-13T11:03:30.114023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"identity_df = pd.read_csv('/kaggle/input/identity-celeba/identity_CelebA.txt', sep='\\s+',  header=None, names=[\"image\", \"identity\"])\nidentity_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:14:12.221246Z","iopub.execute_input":"2025-03-13T11:14:12.221958Z","iopub.status.idle":"2025-03-13T11:14:12.419540Z","shell.execute_reply.started":"2025-03-13T11:14:12.221910Z","shell.execute_reply":"2025-03-13T11:14:12.418396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attributes_df = pd.read_csv(data_dir+'/list_attr_celeba.csv')\nattributes_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:08:19.774685Z","iopub.execute_input":"2025-03-13T11:08:19.775208Z","iopub.status.idle":"2025-03-13T11:08:21.299296Z","shell.execute_reply.started":"2025-03-13T11:08:19.775114Z","shell.execute_reply":"2025-03-13T11:08:21.298240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"partition_df = pd.read_csv(data_dir+'/list_eval_partition.csv')\npartition_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:08:59.762390Z","iopub.execute_input":"2025-03-13T11:08:59.762839Z","iopub.status.idle":"2025-03-13T11:08:59.948194Z","shell.execute_reply.started":"2025-03-13T11:08:59.762805Z","shell.execute_reply":"2025-03-13T11:08:59.947106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"landmarks_df = pd.read_csv(data_dir+'/list_landmarks_align_celeba.csv')\nlandmarks_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:09:28.559080Z","iopub.execute_input":"2025-03-13T11:09:28.559474Z","iopub.status.idle":"2025-03-13T11:09:28.962573Z","shell.execute_reply.started":"2025-03-13T11:09:28.559444Z","shell.execute_reply":"2025-03-13T11:09:28.961432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bbox_df = pd.read_csv(data_dir+'/list_bbox_celeba.csv')\nbbox_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:09:50.880668Z","iopub.execute_input":"2025-03-13T11:09:50.881011Z","iopub.status.idle":"2025-03-13T11:09:51.167779Z","shell.execute_reply.started":"2025-03-13T11:09:50.880983Z","shell.execute_reply":"2025-03-13T11:09:51.166569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Select desired data","metadata":{}},{"cell_type":"code","source":"combined_df = pd.concat([identity_df.set_index(keys=['image'], verify_integrity=True), partition_df.set_index(keys=['image_id'], verify_integrity=True)], axis=1, verify_integrity=True).copy()\ncombined_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:16:00.961658Z","iopub.execute_input":"2025-03-13T11:16:00.962028Z","iopub.status.idle":"2025-03-13T11:16:01.078561Z","shell.execute_reply.started":"2025-03-13T11:16:00.961999Z","shell.execute_reply":"2025-03-13T11:16:01.077407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_df['partition'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:16:14.272209Z","iopub.execute_input":"2025-03-13T11:16:14.272579Z","iopub.status.idle":"2025-03-13T11:16:14.290526Z","shell.execute_reply.started":"2025-03-13T11:16:14.272547Z","shell.execute_reply":"2025-03-13T11:16:14.289088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = combined_df[combined_df['partition']==0].copy()\nprint('Train dataset shape:',train_df.shape)\nval_df = combined_df[combined_df['partition']==1].copy()\nprint('Validation dataset shape:',val_df.shape)\ntest_df = combined_df[combined_df['partition']==2].copy()\nprint('Test dataset shape:',test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:17:49.881533Z","iopub.execute_input":"2025-03-13T11:17:49.881894Z","iopub.status.idle":"2025-03-13T11:17:49.900841Z","shell.execute_reply.started":"2025-03-13T11:17:49.881868Z","shell.execute_reply":"2025-03-13T11:17:49.899804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Dataset","metadata":{}},{"cell_type":"code","source":"def createContrastPairs(data_df):\n  selected_list = []\n  negetive_list = []\n  classes = list(data_df['identity'].unique())\n  for cls1 in tqdm(classes):\n    # randomly select alternate class\n    temp_cls = classes\n    temp_cls.remove(cls1)\n    cls2 = random.choice(temp_cls)\n\n    # List all the\n    images_class1 = data_df[data_df['identity'] == cls1].index.to_list()\n    images_class2 = data_df[data_df['identity'] == cls2].index.to_list()\n\n    # Create list of all positive combinations\n    for img1, img2 in combinations(images_class1, 2):\n      selected_list.append([img1, img2, 1])\n\n    # Create list of negetive combinations\n    for img1, img2 in product(images_class1, images_class2):\n      negetive_list.append([img1, img2, -1])\n\n  # Balance the positive and negetive list\n  negetive_list = random.sample(negetive_list, len(selected_list))\n  # Combine the selections\n  selected_list.extend(negetive_list)\n\n  # Create Dataframe\n  data_df = pd.DataFrame(selected_list, columns=['image1','image2','similarity'])\n  # Shuffle dataset\n  data_df = data_df.sample(frac = 1)\n\n  return data_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:41:08.232115Z","iopub.execute_input":"2025-03-13T11:41:08.232481Z","iopub.status.idle":"2025-03-13T11:41:08.240143Z","shell.execute_reply.started":"2025-03-13T11:41:08.232451Z","shell.execute_reply":"2025-03-13T11:41:08.238985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pair_df = createContrastPairs(train_df)\nprint('Train Dataset Pair shape:', train_pair_df.shape)\nval_pair_df = createContrastPairs(val_df)\nprint('Validation Dataset Pair shape:', val_pair_df.shape)\ntest_pair_df = createContrastPairs(test_df)\nprint('Test Dataset Pair shape:', test_pair_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:41:09.786644Z","iopub.execute_input":"2025-03-13T11:41:09.787027Z","iopub.status.idle":"2025-03-13T11:41:25.740315Z","shell.execute_reply.started":"2025-03-13T11:41:09.786992Z","shell.execute_reply":"2025-03-13T11:41:25.739186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, image_num_1, image_num_2, labels):\n        self.image_num_1 = image_num_1  # Store paths instead of images\n        self.image_num_2 = image_num_2\n        self.labels = labels\n        self.transform_dataset = ViTImageProcessor.from_pretrained(MODEL_TRANSFORMER, attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n\n    def __len__(self):\n        return len(self.image_num_1)\n\n    def __getitem__(self, idx):\n        img_path_1 = self.image_num_1[idx]\n        img_path_2 = self.image_num_2[idx]\n\n         # Load image 1 when needed\n        image_1 = Image.open(img_path_1).convert(\"RGB\")\n        image_1 = self.transform_dataset(image_1)['pixel_values'][0]\n\n         # Load image 2 when needed\n        image_2 = Image.open(img_path_2).convert(\"RGB\")\n        image_2 = self.transform_dataset(image_2)['pixel_values'][0]\n\n        label = self.labels[idx]\n\n        return {'pixel_values_1': image_1, 'pixel_values_2': image_2, 'labels': label}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:41:25.741756Z","iopub.execute_input":"2025-03-13T11:41:25.742101Z","iopub.status.idle":"2025-03-13T11:41:25.749331Z","shell.execute_reply.started":"2025-03-13T11:41:25.742065Z","shell.execute_reply":"2025-03-13T11:41:25.748106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = CustomDataset(\n        image_num_1=[os.path.join(image_dir, img) for img in train_pair_df.image1.to_list()],\n        image_num_2=[os.path.join(image_dir, img) for img in train_pair_df.image2.to_list()],\n        labels=torch.tensor(train_pair_df.similarity.values, dtype=torch.float32)\n        )\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:18:10.003885Z","iopub.execute_input":"2025-03-13T12:18:10.004283Z","iopub.status.idle":"2025-03-13T12:18:15.746753Z","shell.execute_reply.started":"2025-03-13T12:18:10.004253Z","shell.execute_reply":"2025-03-13T12:18:15.745702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_ds = CustomDataset(\n        image_num_1=[os.path.join(image_dir, img) for img in val_pair_df.image1.to_list()],\n        image_num_2=[os.path.join(image_dir, img) for img in val_pair_df.image2.to_list()],\n        labels=torch.tensor(val_pair_df.similarity.values, dtype=torch.float32)\n        )\nval_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:18:15.748029Z","iopub.execute_input":"2025-03-13T12:18:15.748401Z","iopub.status.idle":"2025-03-13T12:18:16.569050Z","shell.execute_reply.started":"2025-03-13T12:18:15.748362Z","shell.execute_reply":"2025-03-13T12:18:16.568082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test_ds = CustomDataset(\n#        image_num_1=[os.path.join(root_dir, img) for img in test_pair_df.image1.to_list()],\n#        image_num_2=[os.path.join(root_dir, img) for img in test_pair_df.image2.to_list()],\n#        labels=torch.tensor(test_pair_df.similarity.values, dtype=torch.int64)\n#        )\n#test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the Dataloader","metadata":{}},{"cell_type":"code","source":"# Convert images to numpy for visualization\ndef imgshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:17:25.436947Z","iopub.execute_input":"2025-03-13T12:17:25.437345Z","iopub.status.idle":"2025-03-13T12:17:25.442909Z","shell.execute_reply.started":"2025-03-13T12:17:25.437309Z","shell.execute_reply":"2025-03-13T12:17:25.441593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing the dataset\nbatch = next(iter(train_dl))\nprint(batch['pixel_values_1'].shape,batch['pixel_values_1'].shape, batch['labels'].shape)\nimgshow(torchvision.utils.make_grid(batch['pixel_values_1'][0]))\nimgshow(torchvision.utils.make_grid(batch['pixel_values_2'][0]))\nprint(batch['labels'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:18:26.744518Z","iopub.execute_input":"2025-03-13T12:18:26.744944Z","iopub.status.idle":"2025-03-13T12:18:27.618336Z","shell.execute_reply.started":"2025-03-13T12:18:26.744912Z","shell.execute_reply":"2025-03-13T12:18:27.617119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:17:38.836373Z","iopub.execute_input":"2025-03-13T12:17:38.836900Z","iopub.status.idle":"2025-03-13T12:17:38.842334Z","shell.execute_reply.started":"2025-03-13T12:17:38.836852Z","shell.execute_reply":"2025-03-13T12:17:38.841058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hyperparameters = {\n    \"learning_rate\": 0.0001,\n    \"num_epochs\": 20, # set to very high number\n    \"seed\": SEED,\n    \"patience\": 10, # early stopping\n    \n    \"num_hidden_layers\": 2,\n    \"num_attention_heads\": 1,\n    \n    \"hidden_dropout_prob\": 0.2,\n    \"attention_probs_dropout_prob\":0.2,\n    \n    \"output_dir_pt\": f\"{output_dir}/vit_celebA_gpu_pt_1.pt\",\n    \"output_dir_transformer\": f\"{output_dir}/vit_celebA_gpu_pt_1\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:17:39.094940Z","iopub.execute_input":"2025-03-13T12:17:39.095323Z","iopub.status.idle":"2025-03-13T12:17:39.100880Z","shell.execute_reply.started":"2025-03-13T12:17:39.095290Z","shell.execute_reply":"2025-03-13T12:17:39.099536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ParallelViTNetwork(torch.nn.Module):\n    def __init__(self):\n        super(ParallelViTNetwork, self).__init__()\n        self.config = ViTConfig.from_pretrained(MODEL_TRANSFORMER ,return_dict=True, \n                                       num_hidden_layers= hyperparameters['num_hidden_layers'],\n                                       num_attention_heads = hyperparameters['num_attention_heads'],\n                                       hidden_dropout_prob = hyperparameters['hidden_dropout_prob'],\n                                       attention_probs_dropout_prob = hyperparameters['attention_probs_dropout_prob']\n                                      )\n        self.embedding_model_1 = ViTModel.from_pretrained(MODEL_TRANSFORMER, config=self.config)\n        self.embedding_model_2 = ViTModel.from_pretrained(MODEL_TRANSFORMER, config=self.config)\n        \n        # Freeze all layers\n        for param in self.embedding_model_1.parameters():\n            param.requires_grad = False\n        for param in self.embedding_model_2.parameters():\n            param.requires_grad = False\n\n        hidden_size = self.embedding_model_1.config.hidden_size\n        self.final_layer = torch.nn.Linear(hidden_size * 2, 1)  # Final dense layer\n        self.activation = torch.nn.Tanh() # To restrict the output between -1 to 1\n\n    def forward(self, image_1, image_2):\n        out1 = self.embedding_model_1(image_1)  # Output from first ViT layer\n        emb1 = out1.last_hidden_state[:, 0, :]\n        \n        out2 = self.embedding_model_2(image_2)  # Output from second ViT layer\n        emb2 = out2.last_hidden_state[:, 0, :]\n        \n        concatenated = torch.cat((emb1, emb2), dim=1)  # Concatenate along the feature dimension\n        output = self.final_layer(concatenated)  # Pass through final dense layer\n        output = self.activation(output)  # Apply Tanh activation\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:18:37.208086Z","iopub.execute_input":"2025-03-13T12:18:37.208482Z","iopub.status.idle":"2025-03-13T12:18:37.217830Z","shell.execute_reply.started":"2025-03-13T12:18:37.208446Z","shell.execute_reply":"2025-03-13T12:18:37.216579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now we train the model\ndef training_function():\n    # Initialize accelerator\n    accelerator = Accelerator()\n    \n    # The seed need to be set before we instantiate the model, as it will determine the random head.\n    set_seed(hyperparameters[\"seed\"])\n    \n    # Instantiate the model, chnage the final classification layer, let Accelerate handle the device placement.\n    embedding_model = ParallelViTNetwork()\n    \n    # Loss function\n    criterion = MSELoss() #CosineEmbeddingLoss(margin=0.25)\n    \n    # Instantiate optimizer\n    optimizer = AdamW(embedding_model.parameters(), lr=hyperparameters[\"learning_rate\"])\n    \n    # Define the learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, verbose=True, min_lr=0.00001\n    )\n    \n    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n    # prepare method.\n    accelerated_model, acclerated_criterion ,acclerated_optimizer, acclerated_train_dl, acclerated_val_dl = accelerator.prepare(embedding_model, criterion, optimizer, train_dl, val_dl)\n    \n    # Build the training loop\n    epochs_no_improve = 0\n    min_val_loss = float(\"inf\")\n\n    for epoch in range(hyperparameters[\"num_epochs\"]):\n        # We only enable the progress bar on the main process to avoid having 8 progress bars.\n        progress_bar = tqdm(range(len(acclerated_train_dl)), disable=not accelerator.is_main_process)\n        progress_bar.set_description(f\"Epoch: {epoch}\")\n        accelerated_model.train()\n        training_loss = []\n        for batch in acclerated_train_dl:\n            # Forward pass\n            img1 = batch['pixel_values_1']\n            img2 = batch['pixel_values_2']\n            label = batch['labels']\n            \n            # Extract embeddings\n            train_output = accelerated_model(img1, img2)\n            \n            # Compute loss\n            train_loss = acclerated_criterion(train_output, label)\n            \n            # Backward pass\n            accelerator.backward(train_loss)\n            \n            # Optimize\n            acclerated_optimizer.step()\n            acclerated_optimizer.zero_grad()\n            \n            # We gather the loss from the GPU cores to have them all.\n            training_loss.append(accelerator.gather(train_loss[None]))\n            progress_bar.set_postfix({'loss': train_loss.item()})\n            progress_bar.update(1)\n\n        # Compute average training loss\n        training_loss_final = torch.stack(training_loss).sum().item() / len(training_loss)\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}: learning rate:\", scheduler.get_last_lr())\n        accelerator.print(f\"epoch {epoch}: training loss:\", training_loss_final)\n        \n        # Evaluate at the end of the epoch (distributed evaluation as we have 8 TPU cores)\n        accelerated_model.eval()\n        validation_loss = []\n\n        for batch in acclerated_val_dl:\n            # Forward pass\n            img1 = batch['pixel_values_1']\n            img2 = batch['pixel_values_2']\n            label = batch['labels']\n            \n            with torch.no_grad():\n                val_output = accelerated_model(img1, img2)\n            \n            val_loss = acclerated_criterion(val_output, label)\n            \n            # We gather the loss from the GPU cores to have them all.\n            validation_loss.append(accelerator.gather(val_loss[None]))\n\n        # Compute average validation loss\n        validation_loss_final = torch.stack(validation_loss).sum().item() / len(validation_loss)\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}: validation loss:\", validation_loss_final)\n    \n        # Step the scheduler\n        scheduler.step(validation_loss_final)\n    \n        # Save model with early stopping\n        if validation_loss_final < min_val_loss:\n            epochs_no_improve = 0\n            min_val_loss = validation_loss_final\n            # Save the entire model (including architecture and weights)\n            torch.save(accelerated_model, hyperparameters['output_dir_pt'])\n            accelerated_model.save_pretrained(hyperparameters['output_dir_transformer'])\n            continue\n        else:\n            epochs_no_improve += 1\n            # Check early stopping condition\n            if epochs_no_improve == hyperparameters[\"patience\"]:\n                accelerator.print(\"Early stopping!\")\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:18:37.497678Z","iopub.execute_input":"2025-03-13T12:18:37.498095Z","iopub.status.idle":"2025-03-13T12:18:37.511512Z","shell.execute_reply.started":"2025-03-13T12:18:37.498061Z","shell.execute_reply":"2025-03-13T12:18:37.510161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_function()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T12:18:40.431753Z","iopub.execute_input":"2025-03-13T12:18:40.432091Z","execution_failed":"2025-03-13T12:19:00.605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}